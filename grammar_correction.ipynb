{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T15:37:46.730585Z",
     "start_time": "2024-06-11T15:37:44.050435Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load those needed packages and libraries\n",
    "import torch\n",
    "import optuna\n",
    "import pandas as pd\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# set the device to cuda if available else cpu\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# load the dataset\n",
    "text_df = pd.read_csv(\"Grammar_Correction.csv\")"
   ],
   "id": "7d61b135dabda602",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T15:37:46.739059Z",
     "start_time": "2024-06-11T15:37:46.731836Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# display the first 10 rows of the dataset to understand the data structure and format of the dataset\n",
    "text_df.head(10)"
   ],
   "id": "ae690f08a27d8d48",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   Serial Number         Error Type              Ungrammatical Statement  \\\n",
       "0              1  Verb Tense Errors        I goes to the store everyday.   \n",
       "1              2  Verb Tense Errors  They was playing soccer last night.   \n",
       "2              3  Verb Tense Errors     She have completed her homework.   \n",
       "3              4  Verb Tense Errors            He don't know the answer.   \n",
       "4              5  Verb Tense Errors            The sun rise in the east.   \n",
       "5              6  Verb Tense Errors            I am eat pizza for lunch.   \n",
       "6              7  Verb Tense Errors   The students studies for the exam.   \n",
       "7              8  Verb Tense Errors         The car need to be repaired.   \n",
       "8              9  Verb Tense Errors  She will goes to the party tonight.   \n",
       "9             10  Verb Tense Errors     They watches the movie together.   \n",
       "\n",
       "                       Standard English  \n",
       "0           I go to the store everyday.  \n",
       "1  They were playing soccer last night.  \n",
       "2       She has completed her homework.  \n",
       "3           He doesn't know the answer.  \n",
       "4            The sun rises in the east.  \n",
       "5          I am eating pizza for lunch.  \n",
       "6      The students study for the exam.  \n",
       "7         The car needs to be repaired.  \n",
       "8     She will go to the party tonight.  \n",
       "9        They watch the movie together.  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Serial Number</th>\n",
       "      <th>Error Type</th>\n",
       "      <th>Ungrammatical Statement</th>\n",
       "      <th>Standard English</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Verb Tense Errors</td>\n",
       "      <td>I goes to the store everyday.</td>\n",
       "      <td>I go to the store everyday.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Verb Tense Errors</td>\n",
       "      <td>They was playing soccer last night.</td>\n",
       "      <td>They were playing soccer last night.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Verb Tense Errors</td>\n",
       "      <td>She have completed her homework.</td>\n",
       "      <td>She has completed her homework.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Verb Tense Errors</td>\n",
       "      <td>He don't know the answer.</td>\n",
       "      <td>He doesn't know the answer.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Verb Tense Errors</td>\n",
       "      <td>The sun rise in the east.</td>\n",
       "      <td>The sun rises in the east.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Verb Tense Errors</td>\n",
       "      <td>I am eat pizza for lunch.</td>\n",
       "      <td>I am eating pizza for lunch.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Verb Tense Errors</td>\n",
       "      <td>The students studies for the exam.</td>\n",
       "      <td>The students study for the exam.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Verb Tense Errors</td>\n",
       "      <td>The car need to be repaired.</td>\n",
       "      <td>The car needs to be repaired.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Verb Tense Errors</td>\n",
       "      <td>She will goes to the party tonight.</td>\n",
       "      <td>She will go to the party tonight.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Verb Tense Errors</td>\n",
       "      <td>They watches the movie together.</td>\n",
       "      <td>They watch the movie together.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T15:37:48.758209Z",
     "start_time": "2024-06-11T15:37:46.739912Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load the pre-trained T5 model and tokenizer from the Hugging Face Transformers library and set the shared parameters (as freezing those parameters defined during the pre-trained stage to avoid catastrophic forgetting) to False\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "\n",
    "# set the shared parameters to False\n",
    "for param in model.shared.parameters():\n",
    "    param.requires_grad = False"
   ],
   "id": "9c41ad6f75f6fc66",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T15:37:48.762283Z",
     "start_time": "2024-06-11T15:37:48.759311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# define the GrammarCorrectionDataset class for the dataset\n",
    "class GrammarCorrectionDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    # return the ungrammatical statement and the standard English statement\n",
    "    def __getitem__(self, idx):\n",
    "        ungrammatical_statement = self.dataframe.iloc[idx]['Ungrammatical Statement']\n",
    "        standard_english = self.dataframe.iloc[idx]['Standard English']\n",
    "        return ungrammatical_statement, standard_english"
   ],
   "id": "9ec3bb232ba141ae",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T15:37:48.768102Z",
     "start_time": "2024-06-11T15:37:48.764174Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# split the dataset into training, validation, and test sets with 80%, 10%, and 10%\n",
    "train_df, test_df = train_test_split(text_df, test_size=0.2)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1)\n",
    "\n",
    "# create the GrammarCorrectionDataset for the training, validation, and test sets\n",
    "train_dataset = GrammarCorrectionDataset(train_df)\n",
    "val_dataset = GrammarCorrectionDataset(val_df)\n",
    "test_dataset = GrammarCorrectionDataset(test_df)"
   ],
   "id": "3074d01ab5682671",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T15:37:49.310316Z",
     "start_time": "2024-06-11T15:37:48.768812Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# create the DataLoaders for the training, validation, and test sets\n",
    "model = model.to(device)\n",
    "# set the model to training mode\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "# initialize the scheduler\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.1)"
   ],
   "id": "4778aa0c63dd18c2",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T15:37:49.315003Z",
     "start_time": "2024-06-11T15:37:49.311226Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# define the train_epoch function to train the model for one epoch\n",
    "def train_epoch(model, dataloader, optimizer, device, scheduler):\n",
    "    # set the model to training mode\n",
    "    model.train()\n",
    "    # initialize the total loss to 0\n",
    "    total_loss = 0\n",
    "    # iterate over the dataloader to get the ungrammatical statement and the standard English statement\n",
    "    for ungrammatical_statement, standard_english in dataloader:\n",
    "        # move the inputs and labels to the device, set the configuration for the tokenizer, and get the outputs from the model\n",
    "        inputs = tokenizer(ungrammatical_statement, return_tensors='pt', padding=True, truncation=True, max_length=256).to(device)\n",
    "        labels = tokenizer(standard_english, return_tensors='pt', padding=True, truncation=True, max_length=256).input_ids.to(device)\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        # get the loss from the outputs and add it to the total loss\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        # backpropagate the loss and update the optimizer\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    # update the scheduler\n",
    "    scheduler.step()\n",
    "    # calculate the average loss\n",
    "    average_loss = total_loss / len(dataloader)\n",
    "    return average_loss"
   ],
   "id": "22bbdbe6d79f6c58",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T18:00:09.744084Z",
     "start_time": "2024-06-11T18:00:09.738876Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# define the eval_model function to evaluate the model on the validation or test set\n",
    "def eval_model(model, dataloader, device):\n",
    "    # set the model to evaluation mode\n",
    "    model.eval()\n",
    "    # initialize the total BLEU score to 0\n",
    "    total_bleu_score = 0\n",
    "    # set the smoothing function to method1\n",
    "    smoothing = SmoothingFunction().method1\n",
    "    with torch.no_grad():\n",
    "        # iterate over the dataloader to get the ungrammatical statement and the standard English statement\n",
    "        for ungrammatical_statement, standard_english in dataloader:\n",
    "            # move the inputs and labels to the device, set the configuration for the tokenizer, and get the outputs from the model\n",
    "            inputs = tokenizer(ungrammatical_statement, return_tensors='pt', padding=True, truncation=True, max_length=256).to(device)\n",
    "            labels = tokenizer(standard_english, return_tensors='pt', padding=True, truncation=True, max_length=256).input_ids.to(device)\n",
    "            outputs = model.generate(**inputs, max_new_tokens=256)\n",
    "            # decode the outputs and calculate the BLEU score\n",
    "            corrected_english = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            reference = [standard_english[0].split()]\n",
    "            candidate = corrected_english.split()\n",
    "            bleu_score = sentence_bleu(reference, candidate, smoothing_function=smoothing)\n",
    "            total_bleu_score += bleu_score\n",
    "    average_bleu_score = total_bleu_score / len(dataloader)\n",
    "    print(average_bleu_score)"
   ],
   "id": "40557478bafa88bb",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-06-11T18:00:10.013082Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# using optuna to tune the hyperparameters of the model and optimizer to improve the BLEU score on the validation set with 10 trials\n",
    "def objective(trial):\n",
    "    # define the hyperparameters to tune\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True)\n",
    "    num_train_epochs = trial.suggest_int(\"num_train_epochs\", 1, 5)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [4, 8, 16, 32])\n",
    "    \n",
    "    # load the pre-trained T5 model and tokenizer from the Hugging Face Transformers library\n",
    "    tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "    model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "    # set the shared parameters to False\n",
    "    for param in model.shared.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # set the optimizer and scheduler\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "    # create the DataLoaders for the training and validation\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # train the model for the specified number of epochs\n",
    "    for epoch in range(num_train_epochs):\n",
    "        # train the model for one epoch and print the average loss\n",
    "        average_loss = train_epoch(model, train_dataloader, optimizer, device, scheduler)\n",
    "        print(f'Epoch: {epoch+1}, Loss: {average_loss}')\n",
    "        eval_model(model, val_dataloader, device)\n",
    "\n",
    "    # evaluate the model with the validation set and return the average BLEU score\n",
    "    average_bleu_score = eval_model(model, test_dataloader, device)\n",
    "    return average_bleu_score\n",
    "\n",
    "# create a study and optimize the objective function with 10 trials\n",
    "study = optuna.create_study(direction=\"maximize\", pruner=optuna.pruners.MedianPruner())\n",
    "study.optimize(objective, n_trials=5)\n",
    "# print the best hyperparameters\n",
    "print(study.best_params)"
   ],
   "id": "aee3c6bbb604ef6b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-11 20:00:10,017] A new study created in memory with name: no-name-96eb8a5e-8412-47ae-93c5-2935835d8a10\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 4.506662003286592\n",
      "0.2997520080932914\n",
      "Epoch: 2, Loss: 3.711914083459875\n",
      "0.3749246073164486\n",
      "Epoch: 3, Loss: 3.628820923658518\n",
      "0.34441158736259625\n",
      "0.3828651977183116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2024-06-11 20:20:31,977] Trial 0 failed with parameters: {'learning_rate': 2.131255340862706e-06, 'num_train_epochs': 3, 'batch_size': 8} because of the following error: The value None could not be cast to float..\n",
      "[W 2024-06-11 20:20:31,977] Trial 0 failed with value None.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 3.0056476839210675\n",
      "0.5239768829946693\n",
      "Epoch: 2, Loss: 0.7515746743782706\n",
      "0.4292118603172004\n",
      "Epoch: 3, Loss: 0.7092621922492981\n",
      "0.20280955936906883\n",
      "0.25447296521390805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2024-06-11 20:29:06,116] Trial 1 failed with parameters: {'learning_rate': 2.8172898468247346e-05, 'num_train_epochs': 3, 'batch_size': 32} because of the following error: The value None could not be cast to float..\n",
      "[W 2024-06-11 20:29:06,116] Trial 1 failed with value None.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 5.329942881406009\n",
      "0.4696742354150895\n",
      "Epoch: 2, Loss: 4.557958634345086\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# load the pre-trained T5 model and tokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "\n",
    "# set the shared parameters to False\n",
    "for param in model.shared.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# set the device to cuda if available else cpu\n",
    "model.to(device)"
   ],
   "id": "d8c6df5d2737f5e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# get the best hyperparameters from the study\n",
    "learning_rate = study.best_params[\"learning_rate\"]\n",
    "num_train_epochs = study.best_params[\"num_train_epochs\"]\n",
    "per_device_train_batch_size = study.best_params[\"per_device_train_batch_size\"]\n",
    "\n",
    "# set the optimizer and scheduler with the best hyperparameters\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "# create the DataLoaders for the training, validation, and test sets with the best hyperparameters (the batch size)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=per_device_train_batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=per_device_train_batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=per_device_train_batch_size, shuffle=True)\n",
    "\n",
    "# train the model for the specified number of epochs with the best hyperparameters and evaluate the model on the validation and test sets\n",
    "for epoch in range(num_train_epochs):\n",
    "    average_loss = train_epoch(model, train_dataloader, optimizer, device, scheduler)\n",
    "    print(f'Epoch: {epoch+1}, Loss: {average_loss}')\n",
    "    eval_model(model, val_dataloader, device)\n",
    "\n",
    "# evaluate the model on the test set with the best hyperparameters and print the average BLEU score\n",
    "eval_model(model, test_dataloader, device)"
   ],
   "id": "d9fbd1e98c2507a6",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
